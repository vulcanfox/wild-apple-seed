{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73e4ae3",
   "metadata": {},
   "source": [
    "# Import packages and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc617738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import nltk as nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tag import CRFTagger\n",
    "import numpy as np\n",
    "import re, unicodedata\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "dataset = load_dataset(\n",
    "    \"tner/bionlp2004\", \n",
    "    cache_dir='./data_cache'\n",
    ")\n",
    "\n",
    "print('Our dataset is a dictionary that has {} splits: \\n\\n{}'.format(len(dataset),dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77172f0",
   "metadata": {},
   "source": [
    "# Formatting the dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efe049",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_sentences_ner = [item['tokens'] for item in dataset['train']]\n",
    "train_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['train']]\n",
    "\n",
    "val_sentences_ner = [item['tokens'] for item in dataset['validation']]\n",
    "val_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['validation']]\n",
    "\n",
    "test_sentences_ner = [item['tokens'] for item in dataset['test']]\n",
    "test_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbc3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of training sentences = {}'.format(len(train_sentences_ner)))\n",
    "print('Number of validation sentences = {}'.format(len(val_sentences_ner)))\n",
    "print('Number of test sentences = {}'.format(len(test_sentences_ner)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e033546",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('An instance from the training set looks like this: \\n\\n{}'.format(train_sentences_ner[101]))\n",
    "print('Corresponding label: \\n\\n{}'.format(train_labels_ner[101]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5897c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique labels: {}'.format(np.unique(np.concatenate(train_labels_ner))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa523db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from labels to the tags\n",
    "\n",
    "all_labels = {\n",
    "    \"O\": 0,\n",
    "    \"B-DNA\": 1,\n",
    "    \"I-DNA\": 2,\n",
    "    \"B-protein\": 3,\n",
    "    \"I-protein\": 4,\n",
    "    \"B-cell_type\": 5,\n",
    "    \"I-cell_type\": 6,\n",
    "    \"B-cell_line\": 7,\n",
    "    \"I-cell_line\": 8,\n",
    "    \"B-RNA\": 9,\n",
    "    \"I-RNA\": 10\n",
    "}\n",
    "\n",
    "mapping = {value:key for key, value in all_labels.items()}\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [list(zip(train_sentences_ner[index],[mapping[int(i)]for i in train_labels_ner[index]]))for index, sentence in enumerate(train_sentences_ner)]\n",
    "\n",
    "val_set = [list(zip(val_sentences_ner[index],[mapping[int(i)]for i in val_labels_ner[index]]))for index, sentence in enumerate(val_sentences_ner)]\n",
    "val_tokens = [tok for tok in val_sentences_ner]\n",
    "val_tags = [[mapping[int(i)]for i in item] for item in val_labels_ner]\n",
    "\n",
    "test_set = [list(zip(test_sentences_ner[index],[mapping[int(i)]for i in test_labels_ner[index]]))for index, sentence in enumerate(test_sentences_ner)]\n",
    "test_tokens = [tok for tok in test_sentences_ner]\n",
    "\n",
    "test_tags = [[mapping[int(i)]for i in item] for item in test_labels_ner]\n",
    "print(val_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26759a",
   "metadata": {},
   "source": [
    "# Create plain CRF model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRFTagger(verbose= True)\n",
    "model.train(train_set,'model.crf.my_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2fe4e7",
   "metadata": {},
   "source": [
    "# Predict on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5b269",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predicted_tags = model.tag_sents(val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aa6acf",
   "metadata": {},
   "source": [
    "# Create functions for fetching the F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb7867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DF to hold F1 scores\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "df = pd.DataFrame(columns=['DNA','protein ','cell_type ','cell_line ','RNA ', 'Macro Avg.'])\n",
    "def append_to_scores_table(scores, model_name):\n",
    "    # Append a placeholder for MacroAvg.\n",
    "    scores.append(0)\n",
    "    df.loc[model_name]= scores\n",
    "    return\n",
    "def append_macro_avg(score, model_name):\n",
    "    df.loc[model_name]['Macro Avg.']= score\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b992a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_fill(index, token, label, spans,start,id_):\n",
    "    # Check if beginning\n",
    "    if 'B-' in label:\n",
    "        start = index\n",
    "        ending = index + 1\n",
    "        named_entity_type = label[2:]\n",
    "    # check if inside\n",
    "    elif 'I-' in label:\n",
    "        ending = index + 1\n",
    "    # check if not entity type\n",
    "    elif start >= 0 and label == 'O':\n",
    "        if named_entity_type not in spans:\n",
    "            spans[named_entity_type] = []\n",
    "        spans[named_entity_type].append((start, ending, id_))\n",
    "        start = -1   \n",
    "    if start >= 0:    \n",
    "        if named_entity_type not in spans:\n",
    "            spans[named_entity_type] = []\n",
    "        spans[named_entity_type].append((start, ending, id_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab90d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_printer(named_entity_types, true_spans, predicted_spans, F1_score_for_each_class, model_name):\n",
    "    \n",
    "    # Manually calculating F1, precision, recall. \n",
    "    for named_entity_type in named_entity_types:\n",
    "        # We loop through all the named entity tpes\n",
    "        # set TP, FN, and FP to zero.\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        false_negative = 0\n",
    "        \n",
    "        for span in true_spans[named_entity_type]:\n",
    "            # check if current true span not in the predicted spans\n",
    "            if span not in predicted_spans[named_entity_type]:\n",
    "                # If so...increment false negative value.\n",
    "                false_negative = false_negative + 1\n",
    "        \n",
    "        for span in predicted_spans[named_entity_type]:\n",
    "            # check if current predicted span in the true spans\n",
    "            if span in true_spans[named_entity_type]:\n",
    "                # If so, increment true positive val\n",
    "                true_positive = true_positive + 1\n",
    "            else:\n",
    "                # otherwise increment false negative val\n",
    "                false_positive = false_positive + 1       \n",
    "        \n",
    "            \n",
    "        if true_positive + false_negative== 0:\n",
    "            # set recall\n",
    "            recall = 0\n",
    "        else:\n",
    "            # calculate recall using TP and FN\n",
    "            recall = true_positive / float(true_positive + false_negative)\n",
    "\n",
    "            \n",
    "        if true_positive + false_positive == 0:\n",
    "            # Set precision\n",
    "            precision = 0\n",
    "        else:\n",
    "            # calculate precision using FP and TP\n",
    "            precision = true_positive / float(false_positive + true_positive)\n",
    "            \n",
    "\n",
    "        if recall + precision == 0:\n",
    "            # Set F1 score\n",
    "            F1 = 0\n",
    "        else:\n",
    "            # Calculate F1 using precision and recall\n",
    "            F1 = 2 * precision * recall / (precision + recall)\n",
    "            \n",
    "\n",
    "        F1_score_for_each_class.append(F1)\n",
    "        print('F1 score for Class: {} = {}'.format(named_entity_type, F1))\n",
    "        \n",
    "    macro_avrg = copy(np.mean(F1_score_for_each_class))\n",
    "    print('Macro averaged F1 score for all classes: {}'.format(np.mean(F1_score_for_each_class)))\n",
    "    append_to_scores_table(F1_score_for_each_class, model_name)\n",
    "    append_macro_avg(macro_avrg, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_spans(tagged_sentences):\n",
    "    # Create a dict to hold spans\n",
    "    spans_dict = {}   \n",
    "    for id_, sentence in enumerate(tagged_sentences):\n",
    "        start = -1\n",
    "        entity_type = None\n",
    "        for index, (token, label) in enumerate(sentence):\n",
    "            span_fill(index, token, label, spans_dict, start, id_)  \n",
    "    return spans_dict\n",
    "\n",
    "def get_f1_scores(test_sents, test_sents_with_pred, model_name):\n",
    "    true_spans = get_spans(test_sents)\n",
    "    predicted_spans = get_spans(test_sents_with_pred)\n",
    "    # A list to hold F1 scores\n",
    "    F1_score_for_each_class = []\n",
    "    # Set named entity types\n",
    "    named_entity_types = true_spans.keys()\n",
    "    \n",
    "    score_printer(named_entity_types, true_spans, predicted_spans, F1_score_for_each_class, model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b57b48",
   "metadata": {},
   "source": [
    "# Get F1 scores for validation set on plain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22978e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_f1_scores(val_set, predicted_tags, 'Plain model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d6fd0",
   "metadata": {},
   "source": [
    "# Create new version of the model that uses previous and next words as additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3708cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Current_next_previous_word_CRFTagger(CRFTagger):\n",
    "    def _get_features(self, toks, i):\n",
    "            tok = toks[i]\n",
    "            # Get features from original method\n",
    "            features = super()._get_features(toks,i)\n",
    "            # Append the current word\n",
    "            features.append(\"CURRENT_WORD\" + tok)\n",
    "            if i < len(toks)-1:\n",
    "                # Append the next word\n",
    "                features.append(\"NEXT_WORD_\" + toks[i+1])\n",
    "                # Append the previous word\n",
    "            if i > 0:\n",
    "                features.append(\"PREVIOUS_WORD_\" + toks[i-1])\n",
    "            return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b81259",
   "metadata": {},
   "source": [
    "# Train the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_word_model = Current_next_previous_word_CRFTagger(verbose=True)\n",
    "multi_word_model.train(train_set, 'model.crf.next_previous_word_CRFTagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ea47b",
   "metadata": {},
   "source": [
    "# Get F1 score for validation set predictions on the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a1781",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = multi_word_model.tag_sents(val_tokens)\n",
    "get_f1_scores(val_set, predicted_tags,'Prev-Next-WRD-Model')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04e8d2",
   "metadata": {},
   "source": [
    "# Create a third model that also uses parts of speech tags in addition to all the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14285a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSBasedTagger(Current_next_previous_word_CRFTagger):\n",
    "    _tokens = None\n",
    "    def _get_features(self, toks, i):\n",
    "        # Adding POS tags as a feature on top of the current features\n",
    "        features = super()._get_features(toks,i)\n",
    "        # Set Pos tagged toks\n",
    "        if toks != self._tokens:\n",
    "            self._pos_tagged_toks = pos_tag(toks)\n",
    "            self._tokens = toks\n",
    "        features.append(self._pos_tagged_toks[i][1])\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f2522",
   "metadata": {},
   "source": [
    "# Instantiate the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSmodel = POSBasedTagger(verbose=True)\n",
    "POSmodel.train(train_set, 'model.crf.POS_Based_Tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262f383d",
   "metadata": {},
   "source": [
    "# Predict on validation set and get F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = POSmodel.tag_sents(val_tokens)\n",
    "get_f1_scores(val_set, predicted_tags,'POS_model')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d113b",
   "metadata": {},
   "source": [
    "# Use the best performing model to predict on unseen (test) data for generalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DF for best performing model on Test Set.\n",
    "df = pd.DataFrame(columns=['DNA','protein ','cell_type ','cell_line ','RNA ', 'Macro Avg.'])\n",
    "# This model performs the best, so let's choose it to predict on unseen (test data) split to see how well it generalises.\n",
    "predicted_tags = multi_word_model.tag_sents(test_tokens)\n",
    "get_f1_scores(test_set, predicted_tags,'Prev-Next-WRD-Model')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922777ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d84a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "data_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
